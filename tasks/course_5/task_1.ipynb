{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-16T17:31:22.910226Z",
     "start_time": "2025-03-16T17:31:22.902445Z"
    }
   },
   "source": [
    "from itertools import product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:31:23.148878Z",
     "start_time": "2025-03-16T17:31:22.970300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)"
   ],
   "id": "7787ce0b8a838f59",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:31:23.181783Z",
     "start_time": "2025-03-16T17:31:23.168249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch_sizes = [32, 128, 512]\n",
    "activation_functions = {'ReLU': nn.ReLU(), 'Sigmoid': nn.Sigmoid(), 'Tanh': nn.Tanh(), 'LeakyReLU': nn.LeakyReLU()}\n",
    "optimizers = {'SGD': optim.SGD, 'Adam': optim.Adam, 'RMSprop': optim.RMSprop}\n",
    "num_epochs = [10, 30, 50]\n",
    "early_stopping_patience = [2, 15]\n",
    "depths = [2, 4, 6]\n",
    "widths = [32, 128, 512]\n",
    "dropouts = [0.2, 0.5]\n",
    "l2_lambdas = [0.001, 0.01]\n",
    "\n",
    "def get_data_loader(batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, test_loader"
   ],
   "id": "5ebb36d9557841c9",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:31:23.252247Z",
     "start_time": "2025-03-16T17:31:23.230288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_mlp(depth, width, activation_fn, dropout=0.0):\n",
    "    layers = []\n",
    "    input_size = 28 * 28\n",
    "    layers.append(nn.Linear(input_size, width))\n",
    "    layers.append(activation_fn)\n",
    "\n",
    "    for _ in range(depth - 1):\n",
    "        if dropout > 0:\n",
    "            layers.append(nn.Dropout(dropout))\n",
    "        layers.append(nn.Linear(width, width))\n",
    "        layers.append(activation_fn)\n",
    "\n",
    "    layers.append(nn.Linear(width, 10))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def train_and_evaluate(depth, width, activation, batch_size, optimizer_name, lr=0.001, dropout=0.0, l2_lambda=0.0, early_stopping_patience=None, max_epochs=50):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_loader, test_loader = get_data_loader(batch_size)\n",
    "    model = create_mlp(depth, width, activation_functions[activation], dropout).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optimizers[optimizer_name](model.parameters(), lr=lr, weight_decay=l2_lambda)\n",
    "\n",
    "    best_acc = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                images, labels = images.view(images.size(0), -1).to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "        accuracy = correct / total\n",
    "\n",
    "        if early_stopping_patience:\n",
    "            if accuracy > best_acc:\n",
    "                best_acc = accuracy\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= early_stopping_patience:\n",
    "                    break\n",
    "\n",
    "    return best_acc"
   ],
   "id": "e6fe3674fcb0d72a",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-16T17:42:14.390999Z",
     "start_time": "2025-03-16T17:31:23.316492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "param_grid = product(depths, widths, activation_functions.keys(), batch_sizes, optimizers.keys(), dropouts, l2_lambdas, num_epochs, early_stopping_patience)\n",
    "results = [(depth, width, activation, batch_size, optimizer_name, dropout, l2_lambda, epochs, patience,\n",
    "            train_and_evaluate(depth, width, activation, batch_size, optimizer_name, dropout=dropout, l2_lambda=l2_lambda, early_stopping_patience=patience, max_epochs=epochs))\n",
    "           for depth, width, activation, batch_size, optimizer_name, dropout, l2_lambda, epochs, patience in param_grid]\n",
    "\n",
    "for res in results:\n",
    "    print(f\"Depth: {res[0]}, Width: {res[1]}, Activation: {res[2]}, Batch: {res[3]}, Opt: {res[4]}, Dropout: {res[5]}, L2: {res[6]}, Epochs: {res[7]}, Patience: {res[8]}, Acc: {res[9]:.4f}\")"
   ],
   "id": "dfb46de193d8c3c7",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m product(depths, widths, activation_functions\u001B[38;5;241m.\u001B[39mkeys(), batch_sizes, optimizers\u001B[38;5;241m.\u001B[39mkeys(), dropouts, l2_lambdas, num_epochs, early_stopping_patience)\n\u001B[0;32m----> 2\u001B[0m results \u001B[38;5;241m=\u001B[39m [(depth, width, activation, batch_size, optimizer_name, dropout, l2_lambda, epochs, patience,\n\u001B[1;32m      3\u001B[0m             train_and_evaluate(depth, width, activation, batch_size, optimizer_name, dropout\u001B[38;5;241m=\u001B[39mdropout, l2_lambda\u001B[38;5;241m=\u001B[39ml2_lambda, early_stopping_patience\u001B[38;5;241m=\u001B[39mpatience, max_epochs\u001B[38;5;241m=\u001B[39mepochs))\n\u001B[1;32m      4\u001B[0m            \u001B[38;5;28;01mfor\u001B[39;00m depth, width, activation, batch_size, optimizer_name, dropout, l2_lambda, epochs, patience \u001B[38;5;129;01min\u001B[39;00m param_grid]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDepth: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Width: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Activation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Batch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m3\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Opt: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m4\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Dropout: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m5\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, L2: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m6\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Epochs: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m7\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Patience: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m8\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m9\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[8], line 3\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m      1\u001B[0m param_grid \u001B[38;5;241m=\u001B[39m product(depths, widths, activation_functions\u001B[38;5;241m.\u001B[39mkeys(), batch_sizes, optimizers\u001B[38;5;241m.\u001B[39mkeys(), dropouts, l2_lambdas, num_epochs, early_stopping_patience)\n\u001B[1;32m      2\u001B[0m results \u001B[38;5;241m=\u001B[39m [(depth, width, activation, batch_size, optimizer_name, dropout, l2_lambda, epochs, patience,\n\u001B[0;32m----> 3\u001B[0m             \u001B[43mtrain_and_evaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdepth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwidth\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mactivation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdropout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdropout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ml2_lambda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43ml2_lambda\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mearly_stopping_patience\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpatience\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_epochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepochs\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m      4\u001B[0m            \u001B[38;5;28;01mfor\u001B[39;00m depth, width, activation, batch_size, optimizer_name, dropout, l2_lambda, epochs, patience \u001B[38;5;129;01min\u001B[39;00m param_grid]\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m res \u001B[38;5;129;01min\u001B[39;00m results:\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDepth: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Width: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m1\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Activation: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m2\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Batch: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m3\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Opt: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m4\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Dropout: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m5\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, L2: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m6\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Epochs: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m7\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Patience: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m8\u001B[39m]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, Acc: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mres[\u001B[38;5;241m9\u001B[39m]\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m.4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[7], line 41\u001B[0m, in \u001B[0;36mtrain_and_evaluate\u001B[0;34m(depth, width, activation, batch_size, optimizer_name, lr, dropout, l2_lambda, early_stopping_patience, max_epochs)\u001B[0m\n\u001B[1;32m     39\u001B[0m correct, total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m images, labels \u001B[38;5;129;01min\u001B[39;00m test_loader:\n\u001B[1;32m     42\u001B[0m         images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mview(images\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m     43\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m model(images)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/python_neural_nets-VlmVW8YE/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    705\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    706\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    707\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 708\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    710\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    711\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable\n\u001B[1;32m    712\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    713\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called\n\u001B[1;32m    714\u001B[0m ):\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/python_neural_nets-VlmVW8YE/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    762\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    763\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 764\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    765\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    766\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/python_neural_nets-VlmVW8YE/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/python_neural_nets-VlmVW8YE/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/python_neural_nets-VlmVW8YE/lib/python3.10/site-packages/torchvision/datasets/mnist.py:146\u001B[0m, in \u001B[0;36mMNIST.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    143\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img\u001B[38;5;241m.\u001B[39mnumpy(), mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mL\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 146\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    149\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/.local/share/virtualenvs/python_neural_nets-VlmVW8YE/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m t(img)\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
